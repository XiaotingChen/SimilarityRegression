{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Specify Parameters/Requirements/Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import json\n",
    "\n",
    "from similarityregression import AlignmentTools as alntools\n",
    "from similarityregression import PairwiseAlignment as pwsaln"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### File Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "WorkingOn = 'DNA'\n",
    "\n",
    "loc_CurrentDB = '../CisBP/' + WorkingOn + '/'\n",
    "loc_EScoreOverlaps = loc_CurrentDB + 'Escores/ByFamily/'\n",
    "loc_DBFiles = loc_CurrentDB + '/DBFiles/'\n",
    "loc_DBDAlignments = loc_CurrentDB + 'DomainAlignments/'\n",
    "\n",
    "#Construct Alignments\n",
    "if os.path.isdir(WorkingOn + '/ConstructAlignments/') == False:\n",
    "    os.mkdir(WorkingOn + '/ConstructAlignments/')\n",
    "loc_ConstructAlignments = WorkingOn + '/ConstructAlignments/Unaligned/'\n",
    "if os.path.isdir(loc_ConstructAlignments) == False:\n",
    "    os.mkdir(loc_ConstructAlignments)\n",
    "if os.path.isdir(loc_ConstructAlignments.replace('Unaligned', 'Aligned')) == False:\n",
    "    os.mkdir(loc_ConstructAlignments.replace('Unaligned', 'Aligned'))\n",
    "    \n",
    "#Models\n",
    "loc_ModelsByFamily = WorkingOn + '/ByFamily/'\n",
    "if os.path.isdir(loc_ModelsByFamily) == False:\n",
    "    os.mkdir(loc_ModelsByFamily)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Read DBFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "motifs = pd.read_csv(loc_DBFiles + 'motifs.tab', sep = '\\t', skiprows=[1], index_col=0)\n",
    "motif_features = pd.read_csv(loc_DBFiles + 'motif_features.tab', sep = '\\t', skiprows=[1], index_col=0)\n",
    "domains = pd.read_csv(loc_DBFiles + 'domains.tab', sep = '\\t', skiprows=[1], index_col=0)\n",
    "tf_families = pd.read_csv(loc_DBFiles + 'tf_families.tab', sep = '\\t', skiprows=[1], index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Blacklists & Replacements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "blacklist_BadFamilies = {\n",
    "    tf_families[tf_families['DBDs'] == 'UNKNOWN' ].index[0] : 'UNKNOWN'\n",
    "}\n",
    "\n",
    "#Filter motifs with missing data\n",
    "blacklist_motifs = set(['M00001_1.97d'])\n",
    "motifs = motifs.drop(blacklist_motifs, axis = 0)\n",
    "\n",
    "#Fill the alignment locations\n",
    "dict_DBDAlignments = {\n",
    "}\n",
    "for Domain_ID, info in domains.iterrows():\n",
    "    Pfam_Name = info['Pfam_Name']\n",
    "    #Check if muscle exists \n",
    "    dalnloc = loc_DBDAlignments + Pfam_Name + '.muscle.fa'\n",
    "    if os.path.isfile(dalnloc) == True:\n",
    "        dict_DBDAlignments[Pfam_Name] = dalnloc\n",
    "    else:\n",
    "        dict_DBDAlignments[Pfam_Name] = loc_DBDAlignments + Pfam_Name + '.hmmaln'\n",
    "        \n",
    "#\n",
    "Blacklist_Studies = set() #Set to ['Lam11', 'Barrera2016']  to remove synthetic/mutated constructs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 1) Create unaligned construct sequence files\n",
    "Based on whatever constructs have E-Score Overlaps (e.g. are from PBM or RNAcompete experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MISSING DBD/RBD Domain Info: M00001_1.97d\n"
     ]
    }
   ],
   "source": [
    "for EScoreOverlapFile in glob.glob(loc_EScoreOverlaps + '*'):\n",
    "    EScoreOverlaps = pd.read_csv(EScoreOverlapFile, sep = '\\t')\n",
    "    ID_TFFam = EScoreOverlapFile.split('/')[-1].replace('.EscoreOverlaps.txt', '')\n",
    "    \n",
    "    if ID_TFFam in blacklist_BadFamilies:\n",
    "        continue\n",
    "    \n",
    "    tf_family = tf_families.loc[ID_TFFam]\n",
    "    DBDs = tf_family['DBDs'].split(',')\n",
    "    \n",
    "    AlnDict_ByPfam = {}\n",
    "    PfamAlnLens = {}\n",
    "    \n",
    "    for DBD in DBDs:\n",
    "        AlnDict_ByPfam[DBD] = {}\n",
    "        loc_alnFile = dict_DBDAlignments[DBD]\n",
    "        if 'hmmaln' in loc_alnFile:\n",
    "            #Parse PFams for match positions\n",
    "            alnmnt, matchpos, _, _ = alntools.ParseStockholmWithMatches(loc_alnFile)\n",
    "            for record in alnmnt:\n",
    "                unaln = record.id\n",
    "                aln = str(record.seq)\n",
    "                aln_matchpos = ''\n",
    "                for i in matchpos:\n",
    "                    aln_matchpos += aln[i]\n",
    "                AlnDict_ByPfam[DBD][unaln] = aln_matchpos.upper().replace('.', '-')\n",
    "        else:\n",
    "            for unaln, aln in alntools.FastaIter(loc_alnFile):\n",
    "                AlnDict_ByPfam[DBD][unaln] = aln.upper().replace('.', '-')\n",
    "        PfamAlnLens[DBD] = len(aln)\n",
    "        \n",
    "    JointSeqDict = {}\n",
    "    for currentDBD, currentDBD_dict in AlnDict_ByPfam.items():\n",
    "        for unaln, aln in currentDBD_dict.items():\n",
    "            jointaln = ''\n",
    "            for DBD in DBDs:\n",
    "                if DBD == currentDBD:\n",
    "                    jointaln += aln\n",
    "                else:\n",
    "                    jointaln += '-'*PfamAlnLens[DBD]\n",
    "            JointSeqDict[unaln] = jointaln\n",
    "                \n",
    "    \n",
    "    MIDs = set(list(EScoreOverlaps['MID_x']) + list(EScoreOverlaps['MID_y']))\n",
    "    with open(loc_ConstructAlignments + ID_TFFam + '.txt', 'w') as outf:\n",
    "        for MID in MIDs:\n",
    "            alnseqs = []\n",
    "            if MID in motif_features['Motif_ID'].values:\n",
    "                MID_mfeats = motif_features[motif_features['Motif_ID'] == MID]\n",
    "                for ID_mfeat, mfeat in MID_mfeats.iterrows():\n",
    "                    unaln = mfeat['MotifFeature_Sequence']\n",
    "                    aln = JointSeqDict[unaln]\n",
    "                    alnseqs.append(aln)\n",
    "                joinedseq = ','.join(alnseqs)\n",
    "                outf.write('\\t'.join([MID, joinedseq]) + '\\n')\n",
    "            else:\n",
    "                print '! MISSING DBD/RBD Domain Info:', MID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 2) Align constructs with pairwise DBD/RBD alignment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DNA/ConstructAlignments/Unaligned/F002_1.97d.txt\n",
      "DNA/ConstructAlignments/Unaligned/F007_1.97d.txt\n",
      "DNA/ConstructAlignments/Unaligned/F009_1.97d.txt\n",
      "DNA/ConstructAlignments/Unaligned/F024_1.97d.txt\n",
      "DNA/ConstructAlignments/Unaligned/F026_1.97d.txt\n",
      "DNA/ConstructAlignments/Unaligned/F028_1.97d.txt\n",
      "DNA/ConstructAlignments/Unaligned/F039_1.97d.txt\n",
      "DNA/ConstructAlignments/Unaligned/F050_1.97d.txt\n",
      "DNA/ConstructAlignments/Unaligned/F082_1.97d.txt\n",
      "DNA/ConstructAlignments/Unaligned/F088_1.97d.txt\n",
      "DNA/ConstructAlignments/Unaligned/F091_1.97d.txt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-02192f9b5508>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'EScoreOverlap'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEScoreOverlaps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'EScoreOverlap'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'EClass'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEScoreOverlaps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'EClass'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0mo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Study'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEScoreOverlaps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Study_x'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Study_y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0moutfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/slambert/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/pandas/core/indexing.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1323\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mKeyError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1325\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1326\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/slambert/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/pandas/core/indexing.pyc\u001b[0m in \u001b[0;36m_getitem_tuple\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m    834\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 836\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_lowerdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    837\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexingError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/slambert/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/pandas/core/indexing.pyc\u001b[0m in \u001b[0;36m_getitem_lowerdim\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m    946\u001b[0m         \u001b[0;31m# we may have a nested tuples indexer here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_nested_tuple_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_nested_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m         \u001b[0;31m# we maybe be using a tuple to represent multiple dimensions here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/slambert/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/pandas/core/indexing.pyc\u001b[0m in \u001b[0;36m_getitem_nested_tuple\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   1021\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1022\u001b[0m             \u001b[0mcurrent_ndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1023\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1024\u001b[0m             \u001b[0maxis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/slambert/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/pandas/core/indexing.pyc\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1550\u001b[0m         \u001b[0;31m# fall thru to straight lookup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1551\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_valid_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1552\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/slambert/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/pandas/core/indexing.pyc\u001b[0m in \u001b[0;36m_has_valid_type\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1405\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mis_list_like_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m             \u001b[0;31m# mi is just a passthru\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/slambert/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/pandas/core/indexing.pyc\u001b[0m in \u001b[0;36mis_list_like_indexer\u001b[0;34m(key)\u001b[0m\n\u001b[1;32m   2032\u001b[0m     \u001b[0;31m# allow a list_like, but exclude NamedTuples which can be indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2033\u001b[0m     return is_list_like(key) and not (isinstance(key, tuple) and\n\u001b[0;32m-> 2034\u001b[0;31m                                       type(key) is not tuple)\n\u001b[0m\u001b[1;32m   2035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2036\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for loc_ConstructAlignment in glob.glob(loc_ConstructAlignments + '*'):\n",
    "    print loc_ConstructAlignment\n",
    "    \n",
    "    loc_AlnJSON = loc_ConstructAlignment.replace('Unaligned', 'Aligned')\n",
    "    \n",
    "    #Get Escore Info\n",
    "    Family_ID = loc_ConstructAlignment.split('/')[-1].replace('.txt','')\n",
    "    EScoreOverlapFile = loc_EScoreOverlaps + Family_ID + '.EscoreOverlaps.txt'\n",
    "    EScoreOverlaps = pd.read_csv(EScoreOverlapFile, sep = '\\t', index_col=[0,3]) \n",
    "    \n",
    "    #Read construct sequnences\n",
    "    UnalnDict = {}\n",
    "    with open(loc_ConstructAlignment, 'r') as infile:\n",
    "        for line in infile:\n",
    "            ID, unaln = line.strip().split('\\t')\n",
    "            unaln = unaln.split(',')\n",
    "            UnalnDict[ID] = unaln\n",
    "    IDs = UnalnDict.keys()\n",
    "    IDs.sort()\n",
    "    \n",
    "    #Loop through all pairs of constructs\n",
    "    with open(loc_AlnJSON, 'w') as outfile:\n",
    "        for x, y in itertools.combinations(IDs, 2):\n",
    "            o = pwsaln.AlignDBDArrays((x, UnalnDict[x]), (y, UnalnDict[y]))\n",
    "            o['EScoreOverlap'] = EScoreOverlaps.loc[(x, y), 'EScoreOverlap']\n",
    "            o['EClass'] = EScoreOverlaps.loc[(x, y), 'EClass']\n",
    "            o['Study'] = list(EScoreOverlaps.loc[(x, y), ['Study_x', 'Study_y']])\n",
    "            \n",
    "            outfile.write('\\t'.join([str((x, y)), json.dumps(o)]) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 3) Create training dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing for R: F002_1.97d.txt\n",
      "Parsing for R: F007_1.97d.txt\n",
      "Parsing for R: F009_1.97d.txt\n",
      "Parsing for R: F024_1.97d.txt\n",
      "Parsing for R: F026_1.97d.txt\n",
      "Parsing for R: F028_1.97d.txt\n",
      "Parsing for R: F039_1.97d.txt\n",
      "Parsing for R: F050_1.97d.txt\n",
      "Parsing for R: F082_1.97d.txt\n",
      "Parsing for R: F088_1.97d.txt\n",
      "Parsing for R: F091_1.97d.txt\n"
     ]
    }
   ],
   "source": [
    "for loc_AlnJSON in glob.glob(loc_ConstructAlignments.replace('Unaligned', 'Aligned') + '*'):\n",
    "    FID = loc_AlnJSON.split('/')[-1]\n",
    "    print 'Parsing for R:', FID\n",
    "    \n",
    "    #Open Outputs (and make folders if necessary)\n",
    "    loc_OutputFiles = loc_ModelsByFamily + FID\n",
    "    if os.path.isdir(loc_OutputFiles) == False:\n",
    "        os.mkdir(loc_OutputFiles)\n",
    "    loc_OutputFiles += '/TrainingData/'\n",
    "    if os.path.isdir(loc_OutputFiles) == False:\n",
    "        os.mkdir(loc_OutputFiles)\n",
    "       \n",
    "    Y_Sims_PctID = open(loc_OutputFiles + 'Y_Sims_PctID.csv', 'w')\n",
    "    X_PctID = open(loc_OutputFiles + 'X_PctID.csv','w')\n",
    "    X_AvgB62 = open(loc_OutputFiles + 'X_AvgB62.csv', 'w')\n",
    "    X_PctID_Smooth3 = open(loc_OutputFiles + 'X_PctID_Smooth3.csv', 'w')\n",
    "    X_AvgB62_Smooth3 = open(loc_OutputFiles + 'X_AvgB62_Smooth3.csv', 'w')\n",
    "    \n",
    "    #Loop through alignment info\n",
    "    count = 0\n",
    "    uIDs = set()\n",
    "    IDs = []\n",
    "    with open(loc_AlnJSON, 'r') as infile:\n",
    "        for line in infile:\n",
    "            count += 1\n",
    "            ID, aln = line.strip().split('\\t')\n",
    "            \n",
    "            ID = eval(ID)\n",
    "            IDs.append(ID)\n",
    "            uIDs.add(ID[0])\n",
    "            uIDs.add(ID[1])\n",
    "            \n",
    "            aln = json.loads(aln)\n",
    "            \n",
    "            #Check if Escores are null\n",
    "            if (pd.isnull(aln['EScoreOverlap'])) or (len(Blacklist_Studies.intersection(aln['Study'])) > 0):\n",
    "                continue\n",
    "                \n",
    "            #Check if Excluded studies are in o\n",
    "                \n",
    "            #1) Parse the Y-info\n",
    "            if count == 1:\n",
    "                h = ['MID_x', 'MID_y', 'EScoreOverlap', 'EClass','PctID_L', 'PctID_S', 'ArrayLenDifference', 'MultiAlnFlag']\n",
    "                Y_Sims_PctID.write(','.join(h) + '\\n')\n",
    "            oline = list(ID) \n",
    "            for col in ['EScoreOverlap', 'EClass','PctID_L', 'PctID_S', 'ArrayLenDifference', 'MultiAlnFlag']:\n",
    "                oline.append(aln[col])\n",
    "            Y_Sims_PctID.write(','.join(map(str, oline)) + '\\n')\n",
    "            \n",
    "            #2) Parse the X matrices\n",
    "            for filehandle, dictID in zip([X_PctID, X_AvgB62, X_PctID_Smooth3, X_AvgB62_Smooth3], \n",
    "                                      ['ByPos.PctID', 'ByPos.AvgB62', 'ByPos.PctID.Smooth3', 'ByPos.AvgB62.Smooth3']):\n",
    "                if count == 1:\n",
    "                    h = ['MID_x', 'MID_y'] + ['p' + str(x + 1) for x in range(len(aln['ByPos.PctID']))]\n",
    "                    filehandle.write(','.join(h) + '\\n')\n",
    "                oline = list(ID) + map(str, aln[dictID])\n",
    "                filehandle.write(','.join(oline) + '\\n')\n",
    "                \n",
    "    #Close Files\n",
    "    for x in [X_PctID, X_AvgB62, X_PctID_Smooth3, X_AvgB62_Smooth3, Y_Sims_PctID]:\n",
    "        x.close()\n",
    "\n",
    "    #Calculate Testing folds\n",
    "    count = 0\n",
    "    with open(loc_OutputFiles + 'CVTestIndicies_i0.txt', 'w') as outf:\n",
    "        for uID in uIDs:\n",
    "            present_0 = []\n",
    "            count += 1\n",
    "            for i, ID in enumerate(IDs):\n",
    "                if uID in ID:\n",
    "                    present_0.append(i)\n",
    "            oline = [uID] + present_0\n",
    "            outf.write('\\t'.join(map(str, oline)) + '\\n')\n",
    "        #print count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
