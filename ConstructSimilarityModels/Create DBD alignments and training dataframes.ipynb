{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import json\n",
    "import gzip\n",
    "\n",
    "from similarityregression import AlignmentTools as alntools\n",
    "from similarityregression import PairwiseAlignment as pwsaln"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Transcription Factors\n",
    "Specify Parameters/Requirements/Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### File Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "WorkingOn = 'DNA' #CIS-BP\n",
    "\n",
    "useMUSCLEalns = False #Use Muscle-refined alignments when available? Else: use Pfam HMM local alignment\n",
    "\n",
    "loc_CurrentDB = '../CisBP/' + WorkingOn + '/'\n",
    "loc_EScoreOverlaps = loc_CurrentDB + 'Escores/ByFamily/'\n",
    "loc_DBFiles = loc_CurrentDB + '/DBFiles/'\n",
    "loc_DBDAlignments = loc_CurrentDB + 'DomainAlignments/'\n",
    "\n",
    "#Construct Alignments\n",
    "if os.path.isdir(WorkingOn + '/ConstructAlignments/') == False:\n",
    "    os.mkdir(WorkingOn + '/ConstructAlignments/')\n",
    "loc_ConstructAlignments = WorkingOn + '/ConstructAlignments/Unaligned/'\n",
    "if os.path.isdir(loc_ConstructAlignments) == False:\n",
    "    os.mkdir(loc_ConstructAlignments)\n",
    "if os.path.isdir(loc_ConstructAlignments.replace('Unaligned', 'Aligned')) == False:\n",
    "    os.mkdir(loc_ConstructAlignments.replace('Unaligned', 'Aligned'))\n",
    "    \n",
    "#Models\n",
    "loc_ModelsByFamily = WorkingOn + '/ByFamily/'\n",
    "if os.path.isdir(loc_ModelsByFamily) == False:\n",
    "    os.mkdir(loc_ModelsByFamily)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Read DBFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "motifs = pd.read_csv(loc_DBFiles + 'motifs.tab', sep = '\\t', skiprows=[1], index_col=0)\n",
    "motif_features = pd.read_csv(loc_DBFiles + 'motif_features.tab', sep = '\\t', skiprows=[1], index_col=0)\n",
    "domains = pd.read_csv(loc_DBFiles + 'domains.tab', sep = '\\t', skiprows=[1], index_col=0)\n",
    "tf_families = pd.read_csv(loc_DBFiles + 'tf_families.tab', sep = '\\t', skiprows=[1], index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Blacklists, Replacements, Selected DBD Alignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "blacklist_BadFamilies = {\n",
    "    tf_families[tf_families['DBDs'] == 'UNKNOWN' ].index[0] : 'UNKNOWN'\n",
    "}\n",
    "\n",
    "#Filter motifs with missing data\n",
    "blacklist_motifs = set(['M00001_1.97d'])\n",
    "motifs = motifs.drop(blacklist_motifs, axis = 0)\n",
    "\n",
    "#Fill the alignment locations\n",
    "dict_DBDAlignments = {\n",
    "}\n",
    "for Domain_ID, info in domains.iterrows():\n",
    "    Pfam_Name = info['Pfam_Name']\n",
    "    if useMUSCLEalns:\n",
    "        #Check if muscle exists \n",
    "        dalnloc = loc_DBDAlignments + Pfam_Name + '.muscle.fa'\n",
    "        if os.path.isfile(dalnloc) == True:\n",
    "            dict_DBDAlignments[Pfam_Name] = dalnloc\n",
    "        else:\n",
    "            dict_DBDAlignments[Pfam_Name] = loc_DBDAlignments + Pfam_Name + '.hmmaln' #Use Pfam HMM alignment\n",
    "    else:\n",
    "        dict_DBDAlignments[Pfam_Name] = loc_DBDAlignments + Pfam_Name + '.hmmaln'\n",
    "        \n",
    "#Filter datasets for training?\n",
    "Blacklist_Studies = set(['Lam11', 'Barrera2016']) #Common useage set to ['Lam11', 'Barrera2016']  to remove synthetic/mutated constructs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 1) Create unaligned construct sequence files\n",
    "Based on whatever constructs have E-Score Overlaps (e.g. are from PBM experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "! MISSING DBD/RBD Domain Info: M00001_1.97d\n"
     ]
    }
   ],
   "source": [
    "for EScoreOverlapFile in glob.glob(loc_EScoreOverlaps + '*'):\n",
    "    EScoreOverlaps = pd.read_csv(EScoreOverlapFile, sep = '\\t')\n",
    "    ID_TFFam = EScoreOverlapFile.split('/')[-1].replace('.EscoreOverlaps.txt', '')\n",
    "    \n",
    "    if ID_TFFam in blacklist_BadFamilies:\n",
    "        continue\n",
    "    \n",
    "    tf_family = tf_families.loc[ID_TFFam]\n",
    "    DBDs = tf_family['DBDs'].split(',')\n",
    "    \n",
    "    AlnDict_ByPfam = {}\n",
    "    PfamAlnLens = {}\n",
    "    \n",
    "    for DBD in DBDs:\n",
    "        AlnDict_ByPfam[DBD] = {}\n",
    "        loc_alnFile = dict_DBDAlignments[DBD]\n",
    "        if 'hmmaln' in loc_alnFile:\n",
    "            #Parse PFams for match positions\n",
    "            alnmnt, matchpos, _, _ = alntools.ParseStockholmWithMatches(loc_alnFile)\n",
    "            for record in alnmnt:\n",
    "                unaln = record.id\n",
    "                aln = str(record.seq)\n",
    "                aln_matchpos = ''\n",
    "                for i in matchpos:\n",
    "                    aln_matchpos += aln[i]\n",
    "                AlnDict_ByPfam[DBD][unaln] = aln_matchpos.upper().replace('.', '-')\n",
    "        else:\n",
    "            for unaln, aln in alntools.FastaIter(loc_alnFile):\n",
    "                AlnDict_ByPfam[DBD][unaln] = aln.upper().replace('.', '-')\n",
    "        PfamAlnLens[DBD] = len(aln)\n",
    "        \n",
    "    JointSeqDict = {}\n",
    "    for currentDBD, currentDBD_dict in AlnDict_ByPfam.items():\n",
    "        for unaln, aln in currentDBD_dict.items():\n",
    "            jointaln = ''\n",
    "            for DBD in DBDs:\n",
    "                if DBD == currentDBD:\n",
    "                    jointaln += aln\n",
    "                else:\n",
    "                    jointaln += '-'*PfamAlnLens[DBD]\n",
    "            JointSeqDict[unaln] = jointaln\n",
    "                \n",
    "    \n",
    "    MIDs = set(list(EScoreOverlaps['MID_x']) + list(EScoreOverlaps['MID_y']))\n",
    "    with open(loc_ConstructAlignments + ID_TFFam + '.txt', 'w') as outf:\n",
    "        for MID in MIDs:\n",
    "            alnseqs = []\n",
    "            if MID in motif_features['Motif_ID'].values:\n",
    "                MID_mfeats = motif_features[motif_features['Motif_ID'] == MID]\n",
    "                for ID_mfeat, mfeat in MID_mfeats.iterrows():\n",
    "                    unaln = mfeat['MotifFeature_Sequence']\n",
    "                    aln = JointSeqDict[unaln]\n",
    "                    alnseqs.append(aln)\n",
    "                joinedseq = ','.join(alnseqs)\n",
    "                outf.write('\\t'.join([MID, joinedseq]) + '\\n')\n",
    "            else:\n",
    "                print '! MISSING DBD/RBD Domain Info:', MID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 2) Align constructs with pairwise DBD alignment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aligning Constructs + Calculating Alignment Stats: F002_1.97d\n",
      "Aligning Constructs + Calculating Alignment Stats: F007_1.97d\n",
      "Aligning Constructs + Calculating Alignment Stats: F009_1.97d\n",
      "Aligning Constructs + Calculating Alignment Stats: F024_1.97d\n",
      "Aligning Constructs + Calculating Alignment Stats: F026_1.97d\n",
      "Aligning Constructs + Calculating Alignment Stats: F028_1.97d\n",
      "Aligning Constructs + Calculating Alignment Stats: F039_1.97d\n",
      "Aligning Constructs + Calculating Alignment Stats: F050_1.97d\n",
      "Aligning Constructs + Calculating Alignment Stats: F082_1.97d\n",
      "Aligning Constructs + Calculating Alignment Stats: F088_1.97d\n",
      "Aligning Constructs + Calculating Alignment Stats: F091_1.97d\n",
      "Aligning Constructs + Calculating Alignment Stats: F135_1.97d\n",
      "Aligning Constructs + Calculating Alignment Stats: F136_1.97d\n",
      "Aligning Constructs + Calculating Alignment Stats: F140_1.97d\n",
      "Aligning Constructs + Calculating Alignment Stats: F141_1.97d\n",
      "Aligning Constructs + Calculating Alignment Stats: F152_1.97d\n",
      "Aligning Constructs + Calculating Alignment Stats: F158_1.97d\n",
      "Aligning Constructs + Calculating Alignment Stats: F161_1.97d\n",
      "Aligning Constructs + Calculating Alignment Stats: F169_1.97d\n",
      "Aligning Constructs + Calculating Alignment Stats: F170_1.97d\n",
      "Aligning Constructs + Calculating Alignment Stats: F173_1.97d\n",
      "Aligning Constructs + Calculating Alignment Stats: F174_1.97d\n",
      "Aligning Constructs + Calculating Alignment Stats: F179_1.97d\n",
      "Aligning Constructs + Calculating Alignment Stats: F180_1.97d\n",
      "Aligning Constructs + Calculating Alignment Stats: F196_1.97d\n",
      "Aligning Constructs + Calculating Alignment Stats: F201_1.97d\n",
      "Aligning Constructs + Calculating Alignment Stats: F212_1.97d\n",
      "Aligning Constructs + Calculating Alignment Stats: F213_1.97d\n",
      "Aligning Constructs + Calculating Alignment Stats: F223_1.97d\n",
      "Aligning Constructs + Calculating Alignment Stats: F230_1.97d\n",
      "Aligning Constructs + Calculating Alignment Stats: F231_1.97d\n",
      "Aligning Constructs + Calculating Alignment Stats: F238_1.97d\n",
      "Aligning Constructs + Calculating Alignment Stats: F241_1.97d\n",
      "Aligning Constructs + Calculating Alignment Stats: F243_1.97d\n",
      "Aligning Constructs + Calculating Alignment Stats: F249_1.97d\n",
      "Aligning Constructs + Calculating Alignment Stats: F251_1.97d\n",
      "Aligning Constructs + Calculating Alignment Stats: F256_1.97d\n",
      "Aligning Constructs + Calculating Alignment Stats: F266_1.97d\n",
      "Aligning Constructs + Calculating Alignment Stats: F273_1.97d\n",
      "Aligning Constructs + Calculating Alignment Stats: F275_1.97d\n",
      "Aligning Constructs + Calculating Alignment Stats: F278_1.97d\n",
      "Aligning Constructs + Calculating Alignment Stats: F281_1.97d\n",
      "Aligning Constructs + Calculating Alignment Stats: F282_1.97d\n",
      "Aligning Constructs + Calculating Alignment Stats: F291_1.97d\n",
      "Aligning Constructs + Calculating Alignment Stats: F293_1.97d\n",
      "Aligning Constructs + Calculating Alignment Stats: F294_1.97d\n",
      "Aligning Constructs + Calculating Alignment Stats: F296_1.97d\n",
      "Aligning Constructs + Calculating Alignment Stats: F299_1.97d\n",
      "Aligning Constructs + Calculating Alignment Stats: F301_1.97d\n",
      "Aligning Constructs + Calculating Alignment Stats: F304_1.97d\n",
      "Aligning Constructs + Calculating Alignment Stats: F305_1.97d\n",
      "Aligning Constructs + Calculating Alignment Stats: F309_1.97d\n",
      "Aligning Constructs + Calculating Alignment Stats: F310_1.97d\n",
      "Aligning Constructs + Calculating Alignment Stats: F312_1.97d\n",
      "Aligning Constructs + Calculating Alignment Stats: F313_1.97d\n",
      "Aligning Constructs + Calculating Alignment Stats: F314_1.97d\n",
      "Aligning Constructs + Calculating Alignment Stats: F315_1.97d\n",
      "Aligning Constructs + Calculating Alignment Stats: F317_1.97d\n",
      "Aligning Constructs + Calculating Alignment Stats: F318_1.97d\n",
      "Aligning Constructs + Calculating Alignment Stats: F320_1.97d\n",
      "Aligning Constructs + Calculating Alignment Stats: F322_1.97d\n",
      "Aligning Constructs + Calculating Alignment Stats: F323_1.97d\n",
      "Aligning Constructs + Calculating Alignment Stats: F324_1.97d\n"
     ]
    }
   ],
   "source": [
    "for loc_ConstructAlignment in glob.glob(loc_ConstructAlignments + '*'):    \n",
    "    loc_AlnJSON = loc_ConstructAlignment.replace('Unaligned', 'Aligned')\n",
    "    \n",
    "    #Get Escore Info\n",
    "    Family_ID = loc_ConstructAlignment.split('/')[-1].replace('.txt','')\n",
    "    EScoreOverlapFile = loc_EScoreOverlaps + Family_ID + '.EscoreOverlaps.txt'\n",
    "    EScoreOverlaps = pd.read_csv(EScoreOverlapFile, sep = '\\t', index_col=[0,3]) \n",
    "    print 'Aligning Constructs + Calculating Alignment Stats:', Family_ID\n",
    "\n",
    "    #Read construct sequnences\n",
    "    UnalnDict = {}\n",
    "    with open(loc_ConstructAlignment, 'r') as infile:\n",
    "        for line in infile:\n",
    "            ID, unaln = line.strip().split('\\t')\n",
    "            unaln = unaln.split(',')\n",
    "            UnalnDict[ID] = unaln\n",
    "    IDs = UnalnDict.keys()\n",
    "    IDs.sort()\n",
    "    \n",
    "    #Loop through all pairs of constructs\n",
    "    with gzip.open(loc_AlnJSON + '.gz', 'w') as outfile:\n",
    "        for x, y in itertools.combinations(IDs, 2):\n",
    "            o = pwsaln.AlignDBDArrays((x, UnalnDict[x]), (y, UnalnDict[y]), ByPosNorm = 'L')\n",
    "            o['EScoreOverlap'] = EScoreOverlaps.loc[(x, y), 'EScoreOverlap']\n",
    "            o['EClass'] = EScoreOverlaps.loc[(x, y), 'EClass']\n",
    "            o['Study'] = list(EScoreOverlaps.loc[(x, y), ['Study_x', 'Study_y']])\n",
    "            \n",
    "            outfile.write('\\t'.join([str((x, y)), json.dumps(o)]) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 3) Create training dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing Alignments for Training (R): F002_1.97d\n",
      "Parsing Alignments for Training (R): F007_1.97d\n",
      "Parsing Alignments for Training (R): F009_1.97d\n",
      "Parsing Alignments for Training (R): F024_1.97d\n",
      "Parsing Alignments for Training (R): F026_1.97d\n",
      "Parsing Alignments for Training (R): F028_1.97d\n",
      "Parsing Alignments for Training (R): F039_1.97d\n",
      "Parsing Alignments for Training (R): F050_1.97d\n",
      "Parsing Alignments for Training (R): F082_1.97d\n",
      "Parsing Alignments for Training (R): F088_1.97d\n",
      "Parsing Alignments for Training (R): F091_1.97d\n",
      "Parsing Alignments for Training (R): F135_1.97d\n",
      "Parsing Alignments for Training (R): F136_1.97d\n",
      "Parsing Alignments for Training (R): F140_1.97d\n",
      "Parsing Alignments for Training (R): F141_1.97d\n",
      "Parsing Alignments for Training (R): F152_1.97d\n",
      "Parsing Alignments for Training (R): F158_1.97d\n",
      "Parsing Alignments for Training (R): F161_1.97d\n",
      "Parsing Alignments for Training (R): F169_1.97d\n",
      "Parsing Alignments for Training (R): F170_1.97d\n",
      "Parsing Alignments for Training (R): F173_1.97d\n",
      "Parsing Alignments for Training (R): F174_1.97d\n",
      "Parsing Alignments for Training (R): F179_1.97d\n",
      "Parsing Alignments for Training (R): F180_1.97d\n",
      "Parsing Alignments for Training (R): F196_1.97d\n",
      "Parsing Alignments for Training (R): F201_1.97d\n",
      "Parsing Alignments for Training (R): F212_1.97d\n",
      "Parsing Alignments for Training (R): F213_1.97d\n",
      "Parsing Alignments for Training (R): F223_1.97d\n",
      "Parsing Alignments for Training (R): F230_1.97d\n",
      "Parsing Alignments for Training (R): F231_1.97d\n",
      "Parsing Alignments for Training (R): F238_1.97d\n",
      "Parsing Alignments for Training (R): F241_1.97d\n",
      "Parsing Alignments for Training (R): F243_1.97d\n",
      "Parsing Alignments for Training (R): F249_1.97d\n",
      "Parsing Alignments for Training (R): F251_1.97d\n",
      "Parsing Alignments for Training (R): F256_1.97d\n",
      "Parsing Alignments for Training (R): F266_1.97d\n",
      "Parsing Alignments for Training (R): F273_1.97d\n",
      "Parsing Alignments for Training (R): F275_1.97d\n",
      "Parsing Alignments for Training (R): F278_1.97d\n",
      "Parsing Alignments for Training (R): F281_1.97d\n",
      "Parsing Alignments for Training (R): F282_1.97d\n",
      "Parsing Alignments for Training (R): F291_1.97d\n",
      "Parsing Alignments for Training (R): F293_1.97d\n",
      "Parsing Alignments for Training (R): F294_1.97d\n",
      "Parsing Alignments for Training (R): F296_1.97d\n",
      "Parsing Alignments for Training (R): F299_1.97d\n",
      "Parsing Alignments for Training (R): F301_1.97d\n",
      "Parsing Alignments for Training (R): F304_1.97d\n",
      "Parsing Alignments for Training (R): F305_1.97d\n",
      "Parsing Alignments for Training (R): F309_1.97d\n",
      "Parsing Alignments for Training (R): F310_1.97d\n",
      "Parsing Alignments for Training (R): F312_1.97d\n",
      "Parsing Alignments for Training (R): F313_1.97d\n",
      "Parsing Alignments for Training (R): F314_1.97d\n",
      "Parsing Alignments for Training (R): F315_1.97d\n",
      "Parsing Alignments for Training (R): F317_1.97d\n",
      "Parsing Alignments for Training (R): F318_1.97d\n",
      "Parsing Alignments for Training (R): F320_1.97d\n",
      "Parsing Alignments for Training (R): F322_1.97d\n",
      "Parsing Alignments for Training (R): F323_1.97d\n",
      "Parsing Alignments for Training (R): F324_1.97d\n"
     ]
    }
   ],
   "source": [
    "for loc_AlnJSON in glob.glob(loc_ConstructAlignments.replace('Unaligned', 'Aligned') + '*'):\n",
    "    FID = loc_AlnJSON.split('/')[-1].replace('.txt.gz', '')\n",
    "    print 'Parsing Alignments for Training (R):', FID\n",
    "    \n",
    "    #Open Outputs (and make folders if necessary)\n",
    "    loc_OutputFiles = loc_ModelsByFamily + FID\n",
    "    if os.path.isdir(loc_OutputFiles) == False:\n",
    "        os.mkdir(loc_OutputFiles)\n",
    "    loc_OutputFiles += '/TrainingData/'\n",
    "    if os.path.isdir(loc_OutputFiles) == False:\n",
    "        os.mkdir(loc_OutputFiles)\n",
    "       \n",
    "    Y_Sims_PctID = gzip.open(loc_OutputFiles + 'Y_Sims_PctID.csv.gz', 'w')\n",
    "    X_PctID = gzip.open(loc_OutputFiles + 'X_PctID.csv.gz','w')\n",
    "    X_AvgB62 = gzip.open(loc_OutputFiles + 'X_AvgB62.csv.gz', 'w')\n",
    "    X_PctID_Smooth3 = gzip.open(loc_OutputFiles + 'X_PctID_Smooth3.csv.gz', 'w')\n",
    "    X_AvgB62_Smooth3 = gzip.open(loc_OutputFiles + 'X_AvgB62_Smooth3.csv.gz', 'w')\n",
    "    \n",
    "    #Loop through alignment info\n",
    "    count = 0\n",
    "    uIDs = set()\n",
    "    IDs = []\n",
    "    IDs_Blacklisted = set()\n",
    "    with gzip.open(loc_AlnJSON, 'r') as infile:\n",
    "        for line in infile:\n",
    "            ID, aln = line.strip().split('\\t')\n",
    "            ID = eval(ID)\n",
    "            aln = json.loads(aln)\n",
    "            \n",
    "            #Check if Escores are null or in Blacklist Studies to skip\n",
    "            if (pd.isnull(aln['EScoreOverlap'])) or (len(Blacklist_Studies.intersection(aln['Study'])) > 0):\n",
    "                #If blacklisted add to heldout set\n",
    "                if len(Blacklist_Studies.intersection(aln['Study'])) > 0:\n",
    "                    IDs_Blacklisted.add(ID)\n",
    "                continue\n",
    "            \n",
    "            count += 1 #ID passed QC/Inclusion criteria \n",
    "            IDs.append(ID)\n",
    "            uIDs.add(ID[0])\n",
    "            uIDs.add(ID[1])\n",
    "                \n",
    "            #1) Parse the Y-info\n",
    "            if count == 1:\n",
    "                h = ['MID_x', 'MID_y', 'EScoreOverlap', 'EClass','PctID_L', 'PctID_S', 'ArrayLenDifference', 'MultiAlnFlag']\n",
    "                Y_Sims_PctID.write(','.join(h) + '\\n')\n",
    "            oline = list(ID) \n",
    "            for col in ['EScoreOverlap', 'EClass','PctID_L', 'PctID_S', 'ArrayLenDifference', 'MultiAlnFlag']:\n",
    "                oline.append(aln[col])\n",
    "            Y_Sims_PctID.write(','.join(map(str, oline)) + '\\n')\n",
    "            \n",
    "            #2) Parse the X matrices\n",
    "            for filehandle, dictID in zip([X_PctID, X_AvgB62, X_PctID_Smooth3, X_AvgB62_Smooth3], \n",
    "                                      ['ByPos.PctID', 'ByPos.AvgB62', 'ByPos.PctID.Smooth3', 'ByPos.AvgB62.Smooth3']):\n",
    "                if count == 1:\n",
    "                    h = ['MID_x', 'MID_y'] + ['p' + str(x + 1) for x in range(len(aln['ByPos.PctID']))]\n",
    "                    filehandle.write(','.join(h) + '\\n')\n",
    "                oline = list(ID) + map(str, aln[dictID])\n",
    "                filehandle.write(','.join(oline) + '\\n')\n",
    "                \n",
    "    #Close Files\n",
    "    for x in [X_PctID, X_AvgB62, X_PctID_Smooth3, X_AvgB62_Smooth3, Y_Sims_PctID]:\n",
    "        x.close()\n",
    "\n",
    "    #Calculate Testing folds\n",
    "    count = 0\n",
    "    with open(loc_OutputFiles + 'CVTestIndicies_i0.txt', 'w') as outf:\n",
    "        for uID in uIDs:\n",
    "            present_0 = []\n",
    "            count += 1\n",
    "            for i, ID in enumerate(IDs):\n",
    "                if uID in ID:\n",
    "                    present_0.append(i)\n",
    "            oline = [uID] + present_0\n",
    "            outf.write('\\t'.join(map(str, oline)) + '\\n')\n",
    "    \n",
    "    #Parse the IDs_Blacklisted data (for future testing)\n",
    "    ## Open heldout files\n",
    "    if len(IDs_Blacklisted) > 0:\n",
    "        Y_Sims_PctID = gzip.open(loc_OutputFiles + 'Heldout.Y_Sims_PctID.csv.gz', 'w')\n",
    "        X_PctID = gzip.open(loc_OutputFiles + 'Heldout.X_PctID.csv.gz','w')\n",
    "        X_AvgB62 = gzip.open(loc_OutputFiles + 'Heldout.X_AvgB62.csv.gz', 'w')\n",
    "        X_PctID_Smooth3 = gzip.open(loc_OutputFiles + 'Heldout.X_PctID_Smooth3.csv.gz', 'w')\n",
    "        X_AvgB62_Smooth3 = gzip.open(loc_OutputFiles + 'Heldout.X_AvgB62_Smooth3.csv.gz', 'w')\n",
    "        ##Loop through alignments to parse out old info\n",
    "        count = 0\n",
    "        with gzip.open(loc_AlnJSON, 'r') as infile:\n",
    "            for line in infile:\n",
    "                ID, aln = line.strip().split('\\t')\n",
    "                ID = eval(ID)\n",
    "                if ID in IDs_Blacklisted:\n",
    "                    count += 1\n",
    "                    aln = json.loads(aln)\n",
    "                    #1) Parse the Y-info\n",
    "                    if count == 1:\n",
    "                        h = ['MID_x', 'MID_y', 'EScoreOverlap', 'EClass','PctID_L', 'PctID_S', 'ArrayLenDifference', 'MultiAlnFlag']\n",
    "                        Y_Sims_PctID.write(','.join(h) + '\\n')\n",
    "                    oline = list(ID) \n",
    "                    for col in ['EScoreOverlap', 'EClass','PctID_L', 'PctID_S', 'ArrayLenDifference', 'MultiAlnFlag']:\n",
    "                        oline.append(aln[col])\n",
    "                    Y_Sims_PctID.write(','.join(map(str, oline)) + '\\n')\n",
    "\n",
    "                    #2) Parse the X matrices\n",
    "                    for filehandle, dictID in zip([X_PctID, X_AvgB62, X_PctID_Smooth3, X_AvgB62_Smooth3], \n",
    "                                              ['ByPos.PctID', 'ByPos.AvgB62', 'ByPos.PctID.Smooth3', 'ByPos.AvgB62.Smooth3']):\n",
    "                        if count == 1:\n",
    "                            h = ['MID_x', 'MID_y'] + ['p' + str(x + 1) for x in range(len(aln['ByPos.PctID']))]\n",
    "                            filehandle.write(','.join(h) + '\\n')\n",
    "                        oline = list(ID) + map(str, aln[dictID])\n",
    "                        filehandle.write(','.join(oline) + '\\n')\n",
    "        #Close Heldout Files\n",
    "        for x in [X_PctID, X_AvgB62, X_PctID_Smooth3, X_AvgB62_Smooth3, Y_Sims_PctID]:\n",
    "            x.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# RNA-Binding Proteins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "WorkingOn = 'RNA'\n",
    "\n",
    "loc_CurrentDB = '../CisBP/' + WorkingOn + '/'\n",
    "loc_EScoreOverlaps = loc_CurrentDB + 'Escores/ByFamily/'\n",
    "loc_DBFiles = loc_CurrentDB + '/DBFiles/'\n",
    "loc_DBDAlignments = loc_CurrentDB + 'DomainAlignments/'\n",
    "\n",
    "#Construct Alignments\n",
    "if os.path.isdir(WorkingOn + '/ConstructAlignments/') == False:\n",
    "    os.mkdir(WorkingOn + '/ConstructAlignments/')\n",
    "loc_ConstructAlignments = WorkingOn + '/ConstructAlignments/Unaligned/'\n",
    "if os.path.isdir(loc_ConstructAlignments) == False:\n",
    "    os.mkdir(loc_ConstructAlignments)\n",
    "if os.path.isdir(loc_ConstructAlignments.replace('Unaligned', 'Aligned')) == False:\n",
    "    os.mkdir(loc_ConstructAlignments.replace('Unaligned', 'Aligned'))\n",
    "    \n",
    "#Models\n",
    "loc_ModelsByFamily = WorkingOn + '/ByFamily/'\n",
    "if os.path.isdir(loc_ModelsByFamily) == False:\n",
    "    os.mkdir(loc_ModelsByFamily)\n",
    "    \n",
    "motifs = pd.read_csv(loc_DBFiles + 'motifs.tab', sep = '\\t', skiprows=[1], index_col=0)\n",
    "motif_features = pd.read_csv(loc_DBFiles + 'motif_features.tab', sep = '\\t', skiprows=[1], index_col=0)\n",
    "domains = pd.read_csv(loc_DBFiles + 'domains.tab', sep = '\\t', skiprows=[1], index_col=0)\n",
    "tf_families = pd.read_csv(loc_DBFiles + 'tf_families.tab', sep = '\\t', skiprows=[1], index_col=0)\n",
    "\n",
    "blacklist_BadFamilies = {\n",
    "    tf_families[tf_families['DBDs'] == 'UNKNOWN' ].index[0] : 'UNKNOWN'\n",
    "}\n",
    "\n",
    "\n",
    "#Fill the alignment locations\n",
    "dict_DBDAlignments = {\n",
    "}\n",
    "for Domain_ID, info in domains.iterrows():\n",
    "    Pfam_Name = info['Pfam_Name']\n",
    "    #Check if muscle exists \n",
    "    dalnloc = loc_DBDAlignments + Pfam_Name + '.muscle.fa'\n",
    "    if os.path.isfile(dalnloc) == True:\n",
    "        dict_DBDAlignments[Pfam_Name] = dalnloc\n",
    "    else:\n",
    "        dict_DBDAlignments[Pfam_Name] = loc_DBDAlignments + Pfam_Name + '.hmmaln'\n",
    "        \n",
    "#\n",
    "Blacklist_Studies = set() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 1) Create unaligned construct sequence files\n",
    "Based on whatever constructs have E-Score Overlaps (e.g. are from RNAcompete experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for EScoreOverlapFile in glob.glob(loc_EScoreOverlaps + '*.EscoreOverlaps.txt'):\n",
    "    EScoreOverlaps = pd.read_csv(EScoreOverlapFile, sep = '\\t')\n",
    "    ID_TFFam = EScoreOverlapFile.split('/')[-1].replace('.EscoreOverlaps.txt', '')\n",
    "    \n",
    "    if ID_TFFam in blacklist_BadFamilies:\n",
    "        continue\n",
    "    \n",
    "    tf_family = tf_families.loc[ID_TFFam]\n",
    "    DBDs = tf_family['DBDs'].split(',')\n",
    "    \n",
    "    AlnDict_ByPfam = {}\n",
    "    PfamAlnLens = {}\n",
    "    \n",
    "    for DBD in DBDs:\n",
    "        AlnDict_ByPfam[DBD] = {}\n",
    "        loc_alnFile = dict_DBDAlignments[DBD]\n",
    "        if 'hmmaln' in loc_alnFile:\n",
    "            #Parse PFams for match positions\n",
    "            alnmnt, matchpos, _, _ = alntools.ParseStockholmWithMatches(loc_alnFile)\n",
    "            for record in alnmnt:\n",
    "                unaln = record.id\n",
    "                aln = str(record.seq)\n",
    "                aln_matchpos = ''\n",
    "                for i in matchpos:\n",
    "                    aln_matchpos += aln[i]\n",
    "                AlnDict_ByPfam[DBD][unaln] = aln_matchpos.upper().replace('.', '-')\n",
    "        else:\n",
    "            for unaln, aln in alntools.FastaIter(loc_alnFile):\n",
    "                AlnDict_ByPfam[DBD][unaln] = aln.upper().replace('.', '-')\n",
    "        PfamAlnLens[DBD] = len(aln)\n",
    "        \n",
    "    JointSeqDict = {}\n",
    "    for currentDBD, currentDBD_dict in AlnDict_ByPfam.items():\n",
    "        for unaln, aln in currentDBD_dict.items():\n",
    "            jointaln = ''\n",
    "            for DBD in DBDs:\n",
    "                if DBD == currentDBD:\n",
    "                    jointaln += aln\n",
    "                else:\n",
    "                    jointaln += '-'*PfamAlnLens[DBD]\n",
    "            JointSeqDict[unaln] = jointaln\n",
    "                \n",
    "    \n",
    "    MIDs = set(list(EScoreOverlaps['MID_x']) + list(EScoreOverlaps['MID_y']))\n",
    "    with open(loc_ConstructAlignments + ID_TFFam + '.txt', 'w') as outf:\n",
    "        for MID in MIDs:\n",
    "            alnseqs = []\n",
    "            if MID in motif_features['Motif_ID'].values:\n",
    "                MID_mfeats = motif_features[motif_features['Motif_ID'] == MID]\n",
    "                for ID_mfeat, mfeat in MID_mfeats.iterrows():\n",
    "                    unaln = mfeat['MotifFeature_Sequence']\n",
    "                    aln = JointSeqDict[unaln]\n",
    "                    alnseqs.append(aln)\n",
    "                joinedseq = ','.join(alnseqs)\n",
    "                outf.write('\\t'.join([MID, joinedseq]) + '\\n')\n",
    "            else:\n",
    "                print '! MISSING DBD/RBD Domain Info:', MID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 2) Align constructs with pairwise RBD alignment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNA/ConstructAlignments/Unaligned/F28_1.95d.txt\n",
      "RNA/ConstructAlignments/Unaligned/F29_1.95d.txt\n",
      "RNA/ConstructAlignments/Unaligned/F34_1.95d.txt\n",
      "RNA/ConstructAlignments/Unaligned/F39_1.95d.txt\n",
      "RNA/ConstructAlignments/Unaligned/F43_1.95d.txt\n",
      "RNA/ConstructAlignments/Unaligned/F47_1.95d.txt\n",
      "RNA/ConstructAlignments/Unaligned/F53_1.95d.txt\n",
      "RNA/ConstructAlignments/Unaligned/F55_1.95d.txt\n",
      "RNA/ConstructAlignments/Unaligned/F57_1.95d.txt\n",
      "RNA/ConstructAlignments/Unaligned/F61_1.95d.txt\n",
      "RNA/ConstructAlignments/Unaligned/F65_1.95d.txt\n"
     ]
    }
   ],
   "source": [
    "for loc_ConstructAlignment in glob.glob(loc_ConstructAlignments + '*'):\n",
    "    print loc_ConstructAlignment\n",
    "    \n",
    "    loc_AlnJSON = loc_ConstructAlignment.replace('Unaligned', 'Aligned')\n",
    "    \n",
    "    #Get Escore Info\n",
    "    Family_ID = loc_ConstructAlignment.split('/')[-1].replace('.txt','')\n",
    "    EScoreOverlapFile = loc_EScoreOverlaps + Family_ID + '.EscoreOverlaps.txt'\n",
    "    EScoreOverlaps = pd.read_csv(EScoreOverlapFile, sep = '\\t', index_col=[0,3]) \n",
    "    \n",
    "    #Read construct sequnences\n",
    "    UnalnDict = {}\n",
    "    with open(loc_ConstructAlignment, 'r') as infile:\n",
    "        for line in infile:\n",
    "            ID, unaln = line.strip().split('\\t')\n",
    "            unaln = unaln.split(',')\n",
    "            UnalnDict[ID] = unaln\n",
    "    IDs = UnalnDict.keys()\n",
    "    IDs.sort()\n",
    "    \n",
    "    #Loop through all pairs of constructs\n",
    "    with gzip.open(loc_AlnJSON + '.gz', 'w') as outfile:\n",
    "        for x, y in itertools.combinations(IDs, 2):\n",
    "            o = pwsaln.AlignDBDArrays((x, UnalnDict[x]), (y, UnalnDict[y]), ByPosNorm = 'L')\n",
    "            o['EScoreOverlap'] = EScoreOverlaps.loc[(x, y), 'EScoreOverlap']\n",
    "            o['EClass'] = EScoreOverlaps.loc[(x, y), 'EClass']\n",
    "            o['Study'] = list(EScoreOverlaps.loc[(x, y), ['Study_x', 'Study_y']])\n",
    "            \n",
    "            outfile.write('\\t'.join([str((x, y)), json.dumps(o)]) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 3) Create training dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing for R: F28_1.95d\n",
      "Parsing for R: F29_1.95d\n",
      "Parsing for R: F34_1.95d\n",
      "Parsing for R: F39_1.95d\n",
      "Parsing for R: F43_1.95d\n",
      "Parsing for R: F47_1.95d\n",
      "Parsing for R: F53_1.95d\n",
      "Parsing for R: F55_1.95d\n",
      "Parsing for R: F57_1.95d\n",
      "Parsing for R: F61_1.95d\n",
      "Parsing for R: F65_1.95d\n"
     ]
    }
   ],
   "source": [
    "for loc_AlnJSON in glob.glob(loc_ConstructAlignments.replace('Unaligned', 'Aligned') + '*'):\n",
    "    FID = loc_AlnJSON.split('/')[-1].replace('.txt.gz', '')\n",
    "    print 'Parsing for R:', FID\n",
    "    \n",
    "    #Open Outputs (and make folders if necessary)\n",
    "    loc_OutputFiles = loc_ModelsByFamily + FID\n",
    "    if os.path.isdir(loc_OutputFiles) == False:\n",
    "        os.mkdir(loc_OutputFiles)\n",
    "    loc_OutputFiles += '/TrainingData/'\n",
    "    if os.path.isdir(loc_OutputFiles) == False:\n",
    "        os.mkdir(loc_OutputFiles)\n",
    "       \n",
    "    Y_Sims_PctID = gzip.open(loc_OutputFiles + 'Y_Sims_PctID.csv.gz', 'w')\n",
    "    X_PctID = gzip.open(loc_OutputFiles + 'X_PctID.csv.gz','w')\n",
    "    X_AvgB62 = gzip.open(loc_OutputFiles + 'X_AvgB62.csv.gz', 'w')\n",
    "    X_PctID_Smooth3 = gzip.open(loc_OutputFiles + 'X_PctID_Smooth3.csv.gz', 'w')\n",
    "    X_AvgB62_Smooth3 = gzip.open(loc_OutputFiles + 'X_AvgB62_Smooth3.csv.gz', 'w')\n",
    "    \n",
    "    #Loop through alignment info\n",
    "    count = 0\n",
    "    uIDs = set()\n",
    "    IDs = []\n",
    "    with gzip.open(loc_AlnJSON, 'r') as infile:\n",
    "        for line in infile:\n",
    "            count += 1\n",
    "            ID, aln = line.strip().split('\\t')\n",
    "            ID = eval(ID)\n",
    "            aln = json.loads(aln)\n",
    "            \n",
    "            #Check if Escores are null\n",
    "            if (pd.isnull(aln['EScoreOverlap'])) or (len(Blacklist_Studies.intersection(aln['Study'])) > 0):\n",
    "                continue\n",
    "                \n",
    "            IDs.append(ID)\n",
    "            uIDs.add(ID[0])\n",
    "            uIDs.add(ID[1])\n",
    "                \n",
    "            #1) Parse the Y-info\n",
    "            if count == 1:\n",
    "                h = ['MID_x', 'MID_y', 'EScoreOverlap', 'EClass','PctID_L', 'PctID_S', 'ArrayLenDifference', 'MultiAlnFlag']\n",
    "                Y_Sims_PctID.write(','.join(h) + '\\n')\n",
    "            oline = list(ID) \n",
    "            for col in ['EScoreOverlap', 'EClass','PctID_L', 'PctID_S', 'ArrayLenDifference', 'MultiAlnFlag']:\n",
    "                oline.append(aln[col])\n",
    "            Y_Sims_PctID.write(','.join(map(str, oline)) + '\\n')\n",
    "            \n",
    "            #2) Parse the X matrices\n",
    "            for filehandle, dictID in zip([X_PctID, X_AvgB62, X_PctID_Smooth3, X_AvgB62_Smooth3], \n",
    "                                      ['ByPos.PctID', 'ByPos.AvgB62', 'ByPos.PctID.Smooth3', 'ByPos.AvgB62.Smooth3']):\n",
    "                if count == 1:\n",
    "                    h = ['MID_x', 'MID_y'] + ['p' + str(x + 1) for x in range(len(aln['ByPos.PctID']))]\n",
    "                    filehandle.write(','.join(h) + '\\n')\n",
    "                oline = list(ID) + map(str, aln[dictID])\n",
    "                filehandle.write(','.join(oline) + '\\n')\n",
    "                \n",
    "    #Close Files\n",
    "    for x in [X_PctID, X_AvgB62, X_PctID_Smooth3, X_AvgB62_Smooth3, Y_Sims_PctID]:\n",
    "        x.close()\n",
    "\n",
    "    #Calculate Testing folds\n",
    "    count = 0\n",
    "    with open(loc_OutputFiles + 'CVTestIndicies_i0.txt', 'w') as outf:\n",
    "        for uID in uIDs:\n",
    "            present_0 = []\n",
    "            count += 1\n",
    "            for i, ID in enumerate(IDs):\n",
    "                if uID in ID:\n",
    "                    present_0.append(i)\n",
    "            oline = [uID] + present_0\n",
    "            outf.write('\\t'.join(map(str, oline)) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
