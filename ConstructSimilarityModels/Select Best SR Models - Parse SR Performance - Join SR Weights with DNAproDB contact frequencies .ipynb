{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, matthews_corrcoef,fbeta_score, f1_score\n",
    "from similarityregression.PredictSimilarity import ReadSRModel\n",
    "\n",
    "WorkingOn = 'DNA'\n",
    "\n",
    "BaselineMethod = 'PctID_L'\n",
    "\n",
    "#Should models with windowed feature smoothing be accepted?\n",
    "AllowSmooth = False\n",
    "AllowSmooth_Exemptions = set() #['F091_1.97d']\n",
    "\n",
    "loc_ModelPerformances = glob.glob(WorkingOn + '/ByFamily/*/Models/PRThresholdData.csv')\n",
    "\n",
    "tf_families = pd.read_csv('../CisBP/' + WorkingOn + '/DBFiles/tf_families.tab', sep = '\\t', skiprows = [1], index_col=0)\n",
    "tf_families = tf_families.rename(index=str, columns={\"Cutoff\": \"CisBP_Cutoff\"})\n",
    "\n",
    "for loc_ModelPerformance in loc_ModelPerformances:\n",
    "    FID = loc_ModelPerformance.split('/')[2]\n",
    "    loc_YSim = loc_ModelPerformance.replace('Models/PRThresholdData.csv', 'TrainingData/Y_Sims_PctID.csv.gz')\n",
    "    try:\n",
    "        YSims = pd.read_csv(loc_YSim, compression='gzip')\n",
    "        tf_families.loc[FID, 'NumberOfPairs'] = YSims.shape[0]\n",
    "        tf_families.loc[FID, 'Experiments'] = len(set(list(YSims['MID_x']) + list(YSims['MID_y'])))\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Parse/Calculate SR performance metrics \n",
    "Metrics = [Precision, Recall, MCC, F1 (micro/macro)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F009_1.97d\n",
      "F026_1.97d\n",
      "F028_1.97d\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/slambert/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/slambert/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/sklearn/metrics/classification.py:538: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F039_1.97d\n",
      "F082_1.97d\n",
      "F091_1.97d\n",
      "F135_1.97d\n",
      "F169_1.97d\n",
      "F170_1.97d\n",
      "F173_1.97d\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/slambert/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/ipykernel_launcher.py:78: RuntimeWarning: invalid value encountered in divide\n",
      "/Users/slambert/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/Users/slambert/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/ipykernel_launcher.py:89: RuntimeWarning: invalid value encountered in divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F174_1.97d\n",
      "F180_1.97d\n",
      "F196_1.97d\n",
      "F201_1.97d\n",
      "F212_1.97d\n",
      "F223_1.97d\n",
      "F231_1.97d\n",
      "F238_1.97d\n",
      "F243_1.97d\n",
      "F251_1.97d\n",
      "F266_1.97d\n",
      "F273_1.97d\n",
      "F278_1.97d\n",
      "F282_1.97d\n",
      "F294_1.97d\n",
      "F299_1.97d\n",
      "F301_1.97d\n",
      "F305_1.97d\n",
      "F312_1.97d\n",
      "F314_1.97d\n",
      "F315_1.97d\n",
      "F323_1.97d\n",
      "F324_1.97d\n"
     ]
    }
   ],
   "source": [
    "MulticlassPerformances = []\n",
    "ConfusionMats = {}\n",
    "\n",
    "for loc_ModelPerformance in loc_ModelPerformances:\n",
    "    FID = loc_ModelPerformance.split('/')[2]\n",
    "    ConfusionMats[FID]= {}\n",
    "    print FID\n",
    "    #Preds/Labels (Training)\n",
    "    Preds = pd.read_csv(WorkingOn + '/ByFamily/' + FID + '/Models/Predictions_TestSet.csv')\n",
    "    Y = pd.read_csv(WorkingOn + '/ByFamily/' + FID + '/TrainingData/Y_Sims_PctID.csv.gz')\n",
    "    Y['TN'] = Y['EScoreOverlap'] >= 0.2\n",
    "    Y['TN'] = Y.TN.astype(int)\n",
    "    \n",
    "    #Preds/Labels (Final)\n",
    "    FinalPreds = pd.read_csv(WorkingOn + '/ByFamily/' + FID + '/Models/Predictions_FinalModel.csv')\n",
    "    \n",
    "    #Assign True Labels\n",
    "    Preds['True'] = 'Amb'\n",
    "    Preds.loc[Preds['EClass'] == 1, 'True'] = 'HS' \n",
    "    Preds.loc[Preds['TN'] == 0, 'True'] = 'Dis' \n",
    "    Y['True'] = 'Amb'\n",
    "    Y.loc[Y['EClass'] == 1, 'True'] = 'HS' \n",
    "    Y.loc[Y['TN'] == 0, 'True'] = 'Dis'\n",
    "    FinalPreds['True'] = 'Amb'\n",
    "    FinalPreds.loc[FinalPreds['Class'] == 1, 'True'] = 'HS' \n",
    "    FinalPreds.loc[FinalPreds['TN'] == 0, 'True'] = 'Dis' \n",
    "    \n",
    "    #Positives\n",
    "    Performances = pd.read_csv(loc_ModelPerformance)\n",
    "    Performances = Performances[Performances['Model'] != 'PctID_S']\n",
    "    if (AllowSmooth == False) and (FID not in AllowSmooth_Exemptions):\n",
    "        Performances = Performances.loc[['Smooth' not in x for x in Performances['Model']],]\n",
    "    \n",
    "    #Negatives\n",
    "    NPerformances = pd.read_csv(loc_ModelPerformance.replace('PRThresholdData', 'NPVThresholdData'))\n",
    "    \n",
    "    for method in set(Performances['Model']):\n",
    "        #Get Positive Info\n",
    "        Performances_HSim = Performances[(Performances['Model'] == method) &\n",
    "                                         (Performances['Precision_TEST'] >= 0.75) &\n",
    "                                         (Performances['Threshold'].isnull() == False)]\n",
    "        if Performances_HSim.shape[0] >= 1:\n",
    "            Performances_HSim_Selected = Performances_HSim.iloc[0]\n",
    "            thresh_class = Performances_HSim_Selected['Threshold']\n",
    "        else:\n",
    "            thresh_class = None\n",
    "        \n",
    "        #Get Negative Info\n",
    "        NPerformances_Dis = NPerformances[(NPerformances['Model'] == method) &\n",
    "                                          (NPerformances['NPV_Target'] >= 0.95) &\n",
    "                                          (NPerformances['Threshold'] < thresh_class)]\n",
    "        if NPerformances_Dis.shape[0] >= 1:\n",
    "            NPerformances_Dis_Selected = NPerformances_Dis.iloc[0]\n",
    "            thresh_tn = NPerformances_Dis_Selected['Threshold']\n",
    "        else:\n",
    "            thresh_tn = None\n",
    "            \n",
    "        #print FID, method, thresh_class, thresh_tn\n",
    "\n",
    "        if 'PctID_' in method:\n",
    "            cpreds = Y[[method, 'True']].copy()\n",
    "            cpreds.columns = ['Score', 'True']\n",
    "            fpreds = cpreds.copy()\n",
    "        else:\n",
    "            cpreds = Preds[[method, 'True']].copy()\n",
    "            cpreds.columns = ['Score', 'True']\n",
    "            fpreds = FinalPreds[[method, 'True']].copy()\n",
    "            fpreds.columns = ['Score', 'True']\n",
    "            \n",
    "        #Assign Predicted Labels (LOCO-CV Micro-Performance)\n",
    "        cpreds['Predicted'] = 'Amb'\n",
    "        if thresh_class != None:\n",
    "            cpreds.loc[cpreds['Score'] > thresh_class, 'Predicted'] = 'HS' \n",
    "        if thresh_tn != None:\n",
    "            cpreds.loc[cpreds['Score'] < thresh_tn, 'Predicted'] = 'Dis'\n",
    "        \n",
    "        cm = confusion_matrix(cpreds['True'], cpreds['Predicted'], labels=[\"HS\", \"Amb\", \"Dis\"])\n",
    "        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        MCC = matthews_corrcoef(cpreds['True'], cpreds['Predicted'])\n",
    "        F1_macro = fbeta_score(cpreds['True'], cpreds['Predicted'], average='macro', beta=1)\n",
    "        F1_micro = fbeta_score(cpreds['True'], cpreds['Predicted'], average='micro', beta=1)\n",
    "        #print FID, method, MCC, F1_macro, F1_micro\n",
    "        \n",
    "        #Assign Predicted Labels (Final Model Performance (all data))\n",
    "        fpreds['Predicted'] = 'Amb'\n",
    "        fpreds.loc[fpreds['Score'] > thresh_class, 'Predicted'] = 'HS' \n",
    "        fpreds.loc[fpreds['Score'] < thresh_tn, 'Predicted'] = 'Dis'\n",
    "        final_cm = confusion_matrix(fpreds['True'], fpreds['Predicted'], labels=[\"HS\", \"Amb\", \"Dis\"])\n",
    "        final_cm_normalized = final_cm.astype('float') / final_cm.sum(axis=1)[:, np.newaxis]\n",
    "        final_MCC = matthews_corrcoef(fpreds['True'], fpreds['Predicted'])\n",
    "        final_F1_macro = fbeta_score(fpreds['True'], fpreds['Predicted'], average='macro', beta=1)\n",
    "        final_F1_micro = fbeta_score(fpreds['True'], fpreds['Predicted'], average='micro', beta=1)\n",
    "        \n",
    "        #print FID, method, MCC, final_MCC, F1_macro, final_F1_macro, F1_micro, final_F1_micro\n",
    "        performancestats = [FID, method, MCC, final_MCC, F1_macro, final_F1_macro, F1_micro, final_F1_micro]\n",
    "        if thresh_class != None:\n",
    "            performancestats += [Performances_HSim_Selected['Recall_TEST'], Performances_HSim_Selected['Recall_FINAL']]\n",
    "        else:\n",
    "            performancestats += [np.nan, np.nan]\n",
    "        \n",
    "        if thresh_tn != None:\n",
    "            performancestats += [NPerformances_Dis_Selected['NegativeRecall_TEST'], NPerformances_Dis_Selected['NegativeRecall_FINAL']]\n",
    "        else:\n",
    "            performancestats += [np.nan, np.nan]\n",
    "        \n",
    "        performancestats += [thresh_class, thresh_tn]\n",
    "        MulticlassPerformances.append(performancestats)\n",
    "        ConfusionMats[FID][method] = {'Raw.Test': cm, 'Raw.Final':final_cm, 'Norm.Test':cm_normalized, 'Norm.Final':final_cm_normalized}\n",
    "\n",
    "MulticlassPerformances = pd.DataFrame(MulticlassPerformances, columns=['FID', 'Model', 'MCC_TEST', 'MCC_FINAL', 'F1macro_TEST', 'F1macro_FINAL', 'F1micro_TEST', 'F1micro_FINAL', \n",
    "                                                                       'Recall_TEST', 'Recall_FINAL', 'NegativeRecall_TEST', 'NegativeRecall_FINAL', 'Thresh_HSim', 'Thresh_Dis'])\n",
    "MulticlassPerformances = MulticlassPerformances.sort_values(['FID', 'MCC_TEST'], ascending=[True,False])\n",
    "MulticlassPerformances['Family_Name'] = [tf_families['Family_Name'].get(x) for x in MulticlassPerformances['FID']]\n",
    "\n",
    "#Select Best Models by MCC Train\n",
    "MulticlassPerformances.loc[MulticlassPerformances['Model'] == BaselineMethod, 'ModelSelection'] = 'BaselineMethod'\n",
    "for FID, data in MulticlassPerformances.groupby('FID'):\n",
    "    MulticlassPerformances.loc[data.index,'Model.Rank'] = range(1,data.shape[0] + 1) \n",
    "    data = data[(data['Model'] != BaselineMethod) & (data['Thresh_HSim'].isnull() == False)]\n",
    "    if data.shape[0] > 0:\n",
    "        MulticlassPerformances.loc[data.index[0], 'ModelSelection'] = 'SR'\n",
    "MulticlassPerformances.to_csv('MulticlassPerformances_All.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Parse Best SR Models -> Scoring Files\n",
    "### Use %ID when SR is worse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if os.path.isdir(WorkingOn + '/SRModels') == False:\n",
    "    os.mkdir(WorkingOn + '/SRModels')\n",
    "    \n",
    "Overides_ModelSelection = {'F170_1.97d' : 'AvgB62.Logistic'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/slambert/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/ipykernel_launcher.py:55: PerformanceWarning: indexing past lexsort depth may impact performance.\n"
     ]
    }
   ],
   "source": [
    "for FID, data in MulticlassPerformances.groupby('FID'):\n",
    "    data = data[data['Thresh_HSim'].isnull() == False]\n",
    "    #Find the best model\n",
    "    if data.shape[0] > 0:\n",
    "        if FID in Overides_ModelSelection:\n",
    "            SelectedModel = Overides_ModelSelection[FID]\n",
    "            dSM = data[data['Model'] == SelectedModel]\n",
    "            dSM = dSM.iloc[0,]\n",
    "        else:\n",
    "            dSM = data.iloc[0,]\n",
    "            SelectedModel = dSM['Model']\n",
    "    else:\n",
    "        SelectedModel = None\n",
    "    \n",
    "    #Initialize container for selected SR/Alignment model\n",
    "    ScoringModel = {}\n",
    "    #Output Data\n",
    "    if SelectedModel != None:\n",
    "        tf_families.loc[FID, 'Model'] = SelectedModel\n",
    "        ScoringModel['Family_ID'] =  FID\n",
    "        ScoringModel['Family_Name'] =  dSM['Family_Name']\n",
    "        if SelectedModel == 'PctID_L':\n",
    "            ScoringModel['Model.Name'] = 'PctID_L'\n",
    "            ScoringModel['Model.Class'] = 'SequenceIdentity'\n",
    "            ScoringModel['Threshold.HSim'] = dSM['Thresh_HSim']\n",
    "            ScoringModel['Threshold.Dis'] = dSM['Thresh_Dis']\n",
    "        else:\n",
    "            #Add PctID_L as the Baseline.Method into the scoring file\n",
    "            BaselineModel = data.loc[data['ModelSelection'] == 'BaselineMethod',]\n",
    "            BaselineModel = BaselineModel.iloc[0,]\n",
    "            ScoringModel['Baseline'] = {}\n",
    "            ScoringModel['Baseline']['Name'] = 'PctID_L'\n",
    "            ScoringModel['Baseline']['Class'] = 'SequenceIdentity'\n",
    "            ScoringModel['Baseline']['Threshold.HSim'] = BaselineModel['Thresh_HSim']\n",
    "            ScoringModel['Baseline']['Threshold.Dis'] = BaselineModel['Thresh_Dis']\n",
    "            \n",
    "            #Add other model information\n",
    "            ScoringModel['Model.Name'] = SelectedModel\n",
    "            ScoringModel['Model.Class'] = 'SimilarityRegression'\n",
    "            ScoringModel['SR.Features'] = SelectedModel.split('.')[0].replace(' (Smooth3)', '_Smooth3') #For Heldout files\n",
    "            if SelectedModel.split('.')[1] == 'Logistic':\n",
    "                ScoringModel['SR.LogisticTransform'] = True\n",
    "            else:\n",
    "                ScoringModel['SR.LogisticTransform'] = False\n",
    "            ScoringModel['Threshold.HSim'] = dSM['Thresh_HSim']\n",
    "            ScoringModel['Threshold.Dis'] = dSM['Thresh_Dis']\n",
    "            #Read weights\n",
    "            SRCoefs = pd.read_csv(WorkingOn + '/ByFamily/' + FID + '/Models/ModelCoefficents.csv', index_col=0)\n",
    "            SRCoefs = SRCoefs[SelectedModel]\n",
    "            ScoringModel['SR.Intercept'] = SRCoefs[0]\n",
    "            ScoringModel['SR.Weights'] = list(SRCoefs[1:])\n",
    "            #Read feature scaling (mean/sd)\n",
    "            SRXscales = pd.read_csv(WorkingOn + '/ByFamily/' + FID + '/Models/XScales.csv', index_col=[1,2])\n",
    "            SRXscales = SRXscales.iloc[:,1:] #Drop weird index column\n",
    "            SRXscales = SRXscales.loc[SelectedModel.split('.')[0],]\n",
    "            ScoringModel['SR.FeatureScales.mean'] = list(SRXscales.loc['mean',])\n",
    "            ScoringModel['SR.FeatureScales.sd'] = list(SRXscales.loc['sd',])\n",
    "        #Output\n",
    "        with open(WorkingOn + '/SRModels/' + FID + '.json', 'w') as loc_FamilySRModel:\n",
    "            loc_FamilySRModel.write(json.dumps(ScoringModel, indent = 4, sort_keys= True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Select score threshold for families without SR models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "SRmodels = glob.glob(WorkingOn + '/SRModels/*.json')\n",
    "YSims = glob.glob(WorkingOn + '/ByFamily/*/TrainingData/Y_Sims_PctID.csv.gz')\n",
    "\n",
    "FID_HasModel = set()\n",
    "\n",
    "cutoffs_HSim = {}\n",
    "cutoffs_Dis = {}\n",
    "cutoffs_Dis_repNA = {}\n",
    "mediancutoff_Dis_repNAtoMin = {}\n",
    "for loc_SRmodel in SRmodels:\n",
    "    SRmodel = ReadSRModel(loc_SRmodel)\n",
    "    FID = SRmodel['Family_ID']\n",
    "    FID_HasModel.add(FID)\n",
    "    if SRmodel['Model.Class'] == 'SequenceIdentity':\n",
    "        cutoffs_HSim[FID] = SRmodel['Threshold.HSim']\n",
    "        if pd.isnull(SRmodel['Threshold.Dis']) == False:\n",
    "            cutoffs_Dis[FID] = SRmodel['Threshold.Dis']\n",
    "            cutoffs_Dis_repNA[FID] = SRmodel['Threshold.Dis']\n",
    "            mediancutoff_Dis_repNAtoMin[FID] = SRmodel['Threshold.Dis']\n",
    "        else:\n",
    "            cutoffs_Dis_repNA[FID] = 0\n",
    "            cY = pd.read_csv(WorkingOn + '/ByFamily/%s/TrainingData/Y_Sims_PctID.csv.gz'%FID, compression='gzip')\n",
    "            mediancutoff_Dis_repNAtoMin[FID] = cY['PctID_L'].min()\n",
    "    else:\n",
    "        cutoffs_HSim[FID] = SRmodel['Baseline']['Threshold.HSim']\n",
    "        if pd.isnull(SRmodel['Baseline']['Threshold.Dis']) == False:\n",
    "            cutoffs_Dis[FID] = SRmodel['Baseline']['Threshold.Dis']\n",
    "            cutoffs_Dis_repNA[FID] = SRmodel['Baseline']['Threshold.Dis']\n",
    "            mediancutoff_Dis_repNAtoMin[FID] = SRmodel['Threshold.Dis']\n",
    "        else:\n",
    "            cutoffs_Dis_repNA[FID] = 0\n",
    "            cY = pd.read_csv(WorkingOn + '/ByFamily/%s/TrainingData/Y_Sims_PctID.csv.gz'%FID, compression='gzip')\n",
    "            mediancutoff_Dis_repNAtoMin[FID] = cY['PctID_L'].min()\n",
    "        \n",
    "mediancutoff_HSim = np.median(cutoffs_HSim.values())\n",
    "mediancutoff_Dis = np.median(cutoffs_Dis.values())\n",
    "mediancutoff_Dis_repNA = np.median(cutoffs_Dis_repNA.values())\n",
    "mediancutoff_Dis_repNAtoMin = np.median(mediancutoff_Dis_repNAtoMin.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Read Y_Sims\n",
    "\n",
    "count_HasModel = 0\n",
    "count_NoModel = 0\n",
    "\n",
    "for loc_YSim in YSims:\n",
    "    FID = loc_YSim.split('/')[2]\n",
    "    if FID in FID_HasModel:\n",
    "        count_HasModel += 1\n",
    "        if count_HasModel == 1:\n",
    "            Y_HasModel = pd.read_csv(loc_YSim, compression='gzip')\n",
    "            Y_HasModel['Family_ID'] = FID\n",
    "        else:\n",
    "            cY = pd.read_csv(loc_YSim, compression='gzip')\n",
    "            cY['Family_ID'] = FID\n",
    "            Y_HasModel = pd.concat([Y_HasModel, cY])\n",
    "    else:\n",
    "        count_NoModel += 1\n",
    "        if count_NoModel == 1:\n",
    "            try:\n",
    "                Y_NoModel = pd.read_csv(loc_YSim, compression='gzip')\n",
    "                Y_NoModel['Family_ID'] = FID\n",
    "            except:\n",
    "                count_NoModel -= 1\n",
    "        else:\n",
    "            try:\n",
    "                cY = pd.read_csv(loc_YSim, compression='gzip')\n",
    "                cY['Family_ID'] = FID\n",
    "                Y_NoModel = pd.concat([Y_NoModel, cY])\n",
    "            except:\n",
    "                count_NoModel -= 1\n",
    "\n",
    "Y_HasModel['TN'] = 1.0*(Y_HasModel['EScoreOverlap'] <= 0.2)\n",
    "Y_NoModel['TN'] = 1.0*(Y_NoModel['EScoreOverlap'] <= 0.2)\n",
    "\n",
    "Y_NoModel_Positive = Y_NoModel[Y_NoModel['EClass'].isnull() == False]\n",
    "\n",
    "Y_All = pd.concat([Y_HasModel, Y_NoModel])\n",
    "Y_All_Positive = Y_All[Y_All['EClass'].isnull() == False]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Use Median Cutoffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%ID THRESHOLDS | HSim: 0.643462 | Dis: 0.346559 | Dis (Na to 0): 0.248077\n",
      "\n",
      "Training Data (Y_HasModel)\n",
      "HSim Data: Y_HasModel Precision: 0.680440 Recall: 0.197529\n",
      "Dis (Na to 0) Data: Y_HasModel Negative Precision: 0.948671 Negative Recall: 0.379697\n",
      "Dis Data: Y_HasModel Negative Precision: 0.882337 Negative Recall: 0.748154\n",
      "\n",
      "Heldout Data (Y_NoModel)\n",
      "HSim Data: Y_NoModel Precision: 0.616667 Recall: 0.804348\n",
      "Dis Data: Y_NoModel Negative Precision: 0.786667 Negative Recall: 0.343023\n",
      "Dis (Na to 0) Data: Y_NoModel Negative Precision: 0.868421 Recall: 0.191860\n",
      "\n",
      "All Data (Y_All)\n",
      "HSim Data: Y_All Precision: 0.679111 Recall: 0.200389\n",
      "Dis Data: Y_All Negative Precision: 0.882280 Negative Recall: 0.747687\n",
      "Dis (Na to 0) Data: Y_All Negative Precision: 0.948620 Negative Recall: 0.379480\n"
     ]
    }
   ],
   "source": [
    "print '%ID THRESHOLDS', '| HSim: %f'%mediancutoff_HSim, '| Dis: %f'%mediancutoff_Dis, '| Dis (Na to 0): %f'%mediancutoff_Dis_repNA\n",
    " \n",
    "# Try HSim cutoffs \n",
    "HasModel_Positive_Precision = Y_HasModel[Y_HasModel['PctID_L'] >= mediancutoff_HSim]['EClass'].mean()\n",
    "HasModel_Positive_Recall = Y_HasModel[Y_HasModel['PctID_L'] >= mediancutoff_HSim]['EClass'].sum()/Y_HasModel['EClass'].sum()\n",
    "\n",
    "NoModel_Positive_Precision = Y_NoModel_Positive[Y_NoModel_Positive['PctID_L'] >= mediancutoff_HSim]['EClass'].mean()\n",
    "NoModel_Positive_Recall = Y_NoModel_Positive[Y_NoModel_Positive['PctID_L'] >= mediancutoff_HSim]['EClass'].sum()/Y_NoModel_Positive['EClass'].sum()\n",
    "\n",
    "All_Positive_Precision = Y_All_Positive[Y_All_Positive['PctID_L'] >= mediancutoff_HSim]['EClass'].mean()\n",
    "All_Positive_Recall = Y_All_Positive[Y_All_Positive['PctID_L'] >= mediancutoff_HSim]['EClass'].sum()/Y_All_Positive['EClass'].sum()\n",
    "\n",
    "# Try Dis cutoffs\n",
    "HasModel_Negative_Precision = Y_HasModel[Y_HasModel['PctID_L'] < mediancutoff_Dis]['TN'].mean()\n",
    "HasModel_Negative_Recall = Y_HasModel[Y_HasModel['PctID_L'] < mediancutoff_Dis]['TN'].sum()/Y_HasModel['TN'].sum()\n",
    "HasModel_Negative_Precision_repNA = Y_HasModel[Y_HasModel['PctID_L'] < mediancutoff_Dis_repNA]['TN'].mean()\n",
    "HasModel_Negative_Recall_repNA = Y_HasModel[Y_HasModel['PctID_L'] < mediancutoff_Dis_repNA]['TN'].sum()/Y_HasModel['TN'].sum()\n",
    "\n",
    "NoModel_Negative_Precision = Y_NoModel[Y_NoModel['PctID_L'] < mediancutoff_Dis]['TN'].mean()\n",
    "NoModel_Negative_Recall = Y_NoModel[Y_NoModel['PctID_L'] < mediancutoff_Dis]['TN'].sum()/Y_NoModel['TN'].sum()\n",
    "NoModel_Negative_Precision_repNA = Y_NoModel[Y_NoModel['PctID_L'] < mediancutoff_Dis_repNA]['TN'].mean()\n",
    "NoModel_Negative_Recall_repNA = Y_NoModel[Y_NoModel['PctID_L'] < mediancutoff_Dis_repNA]['TN'].sum()/Y_NoModel['TN'].sum()\n",
    "\n",
    "All_Negative_Precision = Y_All[Y_All['PctID_L'] < mediancutoff_Dis]['TN'].mean()\n",
    "All_Negative_Recall = Y_All[Y_All['PctID_L'] < mediancutoff_Dis]['TN'].sum()/Y_All['TN'].sum()\n",
    "All_Negative_Precision_repNA = Y_All[Y_All['PctID_L'] < mediancutoff_Dis_repNA]['TN'].mean()\n",
    "All_Negative_Recall_repNA = Y_All[Y_All['PctID_L'] < mediancutoff_Dis_repNA]['TN'].sum()/Y_All['TN'].sum()\n",
    "\n",
    "#Output\n",
    "\n",
    "print '\\nTraining Data (Y_HasModel)'\n",
    "print 'HSim', 'Data: Y_HasModel', 'Precision: %f' %HasModel_Positive_Precision, 'Recall: %f'%HasModel_Positive_Recall\n",
    "print 'Dis (Na to 0)', 'Data: Y_HasModel', 'Negative Precision: %f' %HasModel_Negative_Precision_repNA, 'Negative Recall: %f'%HasModel_Negative_Recall_repNA\n",
    "print 'Dis', 'Data: Y_HasModel', 'Negative Precision: %f' %HasModel_Negative_Precision, 'Negative Recall: %f'%HasModel_Negative_Recall\n",
    "\n",
    "print '\\nHeldout Data (Y_NoModel)'\n",
    "print 'HSim', 'Data: Y_NoModel', 'Precision: %f' %NoModel_Positive_Precision, 'Recall: %f'%NoModel_Positive_Recall\n",
    "print 'Dis', 'Data: Y_NoModel', 'Negative Precision: %f' %NoModel_Negative_Precision, 'Negative Recall: %f'%NoModel_Negative_Recall\n",
    "print 'Dis (Na to 0)', 'Data: Y_NoModel', 'Negative Precision: %f' %NoModel_Negative_Precision_repNA, 'Recall: %f'%NoModel_Negative_Recall_repNA\n",
    "\n",
    "print '\\nAll Data (Y_All)'\n",
    "print 'HSim', 'Data: Y_All', 'Precision: %f' %All_Positive_Precision, 'Recall: %f'%All_Positive_Recall\n",
    "print 'Dis', 'Data: Y_All', 'Negative Precision: %f' %All_Negative_Precision, 'Negative Recall: %f'%All_Negative_Recall\n",
    "print 'Dis (Na to 0)', 'Data: Y_All', 'Negative Precision: %f' %All_Negative_Precision_repNA, 'Negative Recall: %f'%All_Negative_Recall_repNA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Use 70% Rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HSim Data: Y_HasModel Precision: 0.755388 Recall: 0.144350\n",
      "HSim Data: Y_NoModel Precision: 0.656250 Recall: 0.684783\n",
      "HSim Data: Y_All Precision: 0.752889 Recall: 0.146898\n"
     ]
    }
   ],
   "source": [
    "# Try HSim cutoffs \n",
    "HasModel_Positive_Precision = Y_HasModel[Y_HasModel['PctID_L'] >= 0.7]['EClass'].mean()\n",
    "HasModel_Positive_Recall = Y_HasModel[Y_HasModel['PctID_L'] >= 0.7]['EClass'].sum()/Y_HasModel['EClass'].sum()\n",
    "\n",
    "NoModel_Positive_Precision = Y_NoModel_Positive[Y_NoModel_Positive['PctID_L'] >= 0.7]['EClass'].mean()\n",
    "NoModel_Positive_Recall = Y_NoModel_Positive[Y_NoModel_Positive['PctID_L'] >= 0.7]['EClass'].sum()/Y_NoModel_Positive['EClass'].sum()\n",
    "\n",
    "All_Positive_Precision = Y_All_Positive[Y_All_Positive['PctID_L'] >= 0.7]['EClass'].mean()\n",
    "All_Positive_Recall = Y_All_Positive[Y_All_Positive['PctID_L'] >= 0.7]['EClass'].sum()/Y_All_Positive['EClass'].sum()\n",
    "\n",
    "print 'HSim', 'Data: Y_HasModel', 'Precision: %f' %HasModel_Positive_Precision, 'Recall: %f'%HasModel_Positive_Recall\n",
    "print 'HSim', 'Data: Y_NoModel', 'Precision: %f' %NoModel_Positive_Precision, 'Recall: %f'%NoModel_Positive_Recall\n",
    "print 'HSim', 'Data: Y_All', 'Precision: %f' %All_Positive_Precision, 'Recall: %f'%All_Positive_Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Median cutoffs don't extrapolate to data from small families. \n",
    "### Try establishing an aggregate threshold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HSim Thresholds | HasModel: 0.686747 | NoModel: 0.942857 | All: 0.693878\n",
      "Dis Thresholds | HasModel: 0.245455 | NoModel: 0.204082 | All: 0.245455\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "#Positives\n",
    "PRC_HasModel = pd.DataFrame(list(precision_recall_curve(Y_HasModel['EClass'], Y_HasModel['PctID_L'])), index=['Precision', 'Recall', 'Threshold'])\n",
    "PRC_HasModel = PRC_HasModel.T\n",
    "PRC_HasModel_BestThresh = PRC_HasModel.loc[PRC_HasModel[PRC_HasModel['Precision'] >= 0.75]['Recall'].idxmax()]\n",
    "\n",
    "PRC_NoModel = pd.DataFrame(list(precision_recall_curve(Y_NoModel_Positive['EClass'], Y_NoModel_Positive['PctID_L'])), index=['Precision', 'Recall', 'Threshold'])\n",
    "PRC_NoModel = PRC_NoModel.T\n",
    "PRC_NoModel_BestThresh = PRC_NoModel.loc[PRC_NoModel[PRC_NoModel['Precision'] >= 0.75]['Recall'].idxmax()]\n",
    "\n",
    "PRC_All = pd.DataFrame(list(precision_recall_curve(Y_All_Positive['EClass'], Y_All_Positive['PctID_L'])), index=['Precision', 'Recall', 'Threshold'])\n",
    "PRC_All = PRC_All.T\n",
    "PRC_All_BestThresh = PRC_All.loc[PRC_All[PRC_All['Precision'] >= 0.75]['Recall'].idxmax()]\n",
    "\n",
    "#Negatives\n",
    "Negative_PRC_HasModel = pd.DataFrame(list(precision_recall_curve(Y_HasModel['TN'], -1*Y_HasModel['PctID_L'])), index=['Precision', 'Recall', 'Threshold'])\n",
    "Negative_PRC_HasModel = Negative_PRC_HasModel.T\n",
    "Negative_PRC_HasModel['Threshold'] = -1*Negative_PRC_HasModel['Threshold']\n",
    "Negative_PRC_HasModel_BestThresh = Negative_PRC_HasModel.loc[Negative_PRC_HasModel[Negative_PRC_HasModel['Precision'] >= 0.95]['Recall'].idxmax()]\n",
    "\n",
    "Negative_PRC_NoModel = pd.DataFrame(list(precision_recall_curve(Y_NoModel['TN'], -1*Y_NoModel['PctID_L'])), index=['Precision', 'Recall', 'Threshold'])\n",
    "Negative_PRC_NoModel = Negative_PRC_NoModel.T\n",
    "Negative_PRC_NoModel['Threshold'] = -1*Negative_PRC_NoModel['Threshold']\n",
    "Negative_PRC_NoModel_BestThresh = Negative_PRC_NoModel.loc[Negative_PRC_NoModel[Negative_PRC_NoModel['Precision'] >= 0.95]['Recall'].idxmax()]\n",
    "\n",
    "Negative_PRC_All = pd.DataFrame(list(precision_recall_curve(Y_All['TN'], -1*Y_All['PctID_L'])), index=['Precision', 'Recall', 'Threshold'])\n",
    "Negative_PRC_All = Negative_PRC_All.T\n",
    "Negative_PRC_All['Threshold'] = -1*Negative_PRC_All['Threshold']\n",
    "Negative_PRC_All_BestThresh = Negative_PRC_All.loc[Negative_PRC_All[Negative_PRC_All['Precision'] >= 0.95]['Recall'].idxmax()]\n",
    "\n",
    "print 'HSim Thresholds', '| HasModel: %f'%PRC_HasModel_BestThresh['Threshold'], '| NoModel: %f'%PRC_NoModel_BestThresh['Threshold'], '| All: %f'%PRC_All_BestThresh['Threshold']\n",
    "print 'Dis Thresholds', '| HasModel: %f'%Negative_PRC_HasModel_BestThresh['Threshold'], '| NoModel: %f'%Negative_PRC_NoModel_BestThresh['Threshold'], '| All: %f'%Negative_PRC_All_BestThresh['Threshold']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Ouput %ID (PctID_L) cutoffs for DBDs/TF Families without a SR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### HIGHLY SIMILAR ###\n",
    "#Set to the same as the CIS-BP paper, and higher than \n",
    "#the median cutoff of baseline models\n",
    "FinalThreshold_HSim = 0.7 \n",
    "\n",
    "### Disimilar ###\n",
    "#Set to 25% which is lower than the median cutoff for families \n",
    "#with SR models and aproximitely equivalent to the threshold that \n",
    "#classifies all data with a 95% precision\n",
    "FinalThreshold_Dis = 0.25\n",
    "\n",
    "ScoringModel = {}\n",
    "ScoringModel['Family_ID'] =  None\n",
    "ScoringModel['Family_Name'] =  \"NO_THRESHOLD\"\n",
    "ScoringModel['Model.Name'] = 'PctID_L'\n",
    "ScoringModel['Model.Class'] = 'SequenceIdentity'\n",
    "ScoringModel['Threshold.HSim'] = FinalThreshold_HSim\n",
    "ScoringModel['Threshold.Dis'] = FinalThreshold_Dis\n",
    "\n",
    "with open(WorkingOn + '/SRModels/NO_THRESHOLD.json', 'w') as loc_FamilySRModel:\n",
    "    loc_FamilySRModel.write(json.dumps(ScoringModel, indent = 4, sort_keys= True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Parse Model Improvements (Log2FC, % change, Abs. change ) to Long Form for ggplot2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing: F009_1.97d\n",
      "Parsing: F026_1.97d\n",
      "Parsing: F028_1.97d\n",
      "Parsing: F039_1.97d\n",
      "Parsing: F082_1.97d\n",
      "Parsing: F091_1.97d\n",
      "Parsing: F135_1.97d\n",
      "Parsing: F169_1.97d\n",
      "Parsing: F170_1.97d\n",
      "Parsing: F173_1.97d\n",
      "Parsing: F174_1.97d\n",
      "Parsing: F180_1.97d\n",
      "Parsing: F196_1.97d\n",
      "Parsing: F201_1.97d\n",
      "Parsing: F212_1.97d\n",
      "Parsing: F223_1.97d\n",
      "Parsing: F231_1.97d\n",
      "Parsing: F238_1.97d\n",
      "Parsing: F243_1.97d\n",
      "Parsing: F251_1.97d\n",
      "Parsing: F266_1.97d\n",
      "Parsing: F273_1.97d\n",
      "Parsing: F278_1.97d\n",
      "Parsing: F282_1.97d\n",
      "Parsing: F294_1.97d\n",
      "Parsing: F299_1.97d\n",
      "Parsing: F301_1.97d\n",
      "Parsing: F305_1.97d\n",
      "Parsing: F312_1.97d\n",
      "Parsing: F314_1.97d\n",
      "Parsing: F315_1.97d\n",
      "Parsing: F323_1.97d\n",
      "Parsing: F324_1.97d\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/slambert/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/ipykernel_launcher.py:24: RuntimeWarning: divide by zero encountered in log2\n"
     ]
    }
   ],
   "source": [
    "SelectedModels_LF = []\n",
    "for FID, data in MulticlassPerformances.groupby('FID'):\n",
    "    print 'Parsing:', FID\n",
    "    if ('SR' in list(data['ModelSelection'])) and ('BaselineMethod' in list(data['ModelSelection'])):\n",
    "        SelectedModel = MulticlassPerformances.loc[data[data['ModelSelection'] == 'SR'].index[0]]\n",
    "        BaselineModel = MulticlassPerformances.loc[data[data['ModelSelection'] == 'BaselineMethod'].index[0]]\n",
    "        \n",
    "        rline = [FID, SelectedModel['Family_Name'], SelectedModel['Model']]\n",
    "        rline += SelectedModel['Model'].split('.')\n",
    "        #Recall_FINAL\n",
    "        oline = rline + ['Recall @ 75% Precision', SelectedModel['Recall_FINAL'], BaselineModel['Recall_FINAL']]\n",
    "        SelectedModels_LF.append(oline)\n",
    "        #NegativeRecall_FINAL\n",
    "        oline = rline + ['Specificity @ 95% NPV', SelectedModel['NegativeRecall_FINAL'], BaselineModel['NegativeRecall_FINAL']]\n",
    "        SelectedModels_LF.append(oline)\n",
    "        #MCC_FINAL\n",
    "        oline = rline + ['Matthews correlation coefficient (MCC)', SelectedModel['MCC_FINAL'], BaselineModel['MCC_FINAL']]\n",
    "        SelectedModels_LF.append(oline)\n",
    "        #Number of Pairs\n",
    "        oline = rline + ['Experiments', np.nan, np.nan]\n",
    "        SelectedModels_LF.append(oline)\n",
    "        \n",
    "SelectedModels_LF = pd.DataFrame(SelectedModels_LF, columns=['Family_ID', 'Family_Name', 'SRModel', 'Features', 'Response', 'PerformanceMetric', 'SR', 'PctID'])\n",
    "SelectedModels_LF['Log2FC'] = np.log2(SelectedModels_LF['SR']/SelectedModels_LF['PctID'])\n",
    "SelectedModels_LF['PercentChange'] = 100*(SelectedModels_LF['SR'] - SelectedModels_LF['PctID'])/SelectedModels_LF['PctID']\n",
    "SelectedModels_LF['AbsoluteChange'] = SelectedModels_LF['SR'] - SelectedModels_LF['PctID']\n",
    "\n",
    "for FID in set(SelectedModels_LF['Family_ID']):\n",
    "    numexperiments = tf_families['Experiments'].get(FID)\n",
    "    SelectedModels_LF.loc[(SelectedModels_LF['Family_ID'] == FID) & (SelectedModels_LF['PerformanceMetric'] == 'Experiments'), 'Log2FC'] = numexperiments\n",
    "    SelectedModels_LF.loc[(SelectedModels_LF['Family_ID'] == FID) & (SelectedModels_LF['PerformanceMetric'] == 'Experiments'), 'PercentChange'] = numexperiments\n",
    "    SelectedModels_LF.loc[(SelectedModels_LF['Family_ID'] == FID) & (SelectedModels_LF['PerformanceMetric'] == 'Experiments'), 'AbsoluteChange'] = numexperiments\n",
    "    \n",
    "SelectedModels_LF.to_csv('SelectedModelImprovements_LF.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Compare selected SR Model Weights to DNAproDB Contact Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DM DM\n",
      "TCR/CxC TCR\n",
      "AP2 AP2\n",
      "C2H2 ZF zf-C2H2\n",
      "Nuclear receptor zf-C4\n",
      "Forkhead Forkhead\n",
      "bZIP bZIP_1\n",
      "HSF HSF_DNA-bind\n",
      "E2F E2F_TDP\n",
      "WRKY WRKY\n",
      "Myb/SANT Myb_DNA-binding\n",
      "GATA GATA\n",
      "Ets Ets\n",
      "RFX RFX_DNA_binding\n",
      "GCM GCM\n",
      "NAC/NAM NAM\n",
      "ARID/BRIGHT ARID\n",
      "bHLH HLH\n",
      "APSES KilA-N\n",
      "T-box T-box\n",
      "Zinc cluster Zn_clus\n",
      "AT hook AT_hook\n",
      "Homeodomain Homeobox\n",
      "Pipsqueak HTH_psq\n",
      "Sox HMG_box\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "SelectedModels_LF = pd.read_csv('SelectedModelImprovements_LF.csv')\n",
    "\n",
    "NormCoefsBySD = False\n",
    "\n",
    "Selections = {}\n",
    "for i,d in SelectedModels_LF.iterrows():\n",
    "    Coefs = pd.read_csv(WorkingOn + '/ByFamily/'+ d['Family_ID'] + '/Models/ModelCoefficents.csv', index_col=0)\n",
    "    CoefsOfInterest = Coefs[d['SRModel']]\n",
    "    Features = d['SRModel'].split('.')[0]\n",
    "    if NormCoefsBySD:\n",
    "        Scaling = pd.read_csv(WorkingOn + '/ByFamily/'+ d['Family_ID'] + '/Models/Xscales.csv', index_col=0)\n",
    "        Scaling.set_index(['X','Stat'], inplace = True)\n",
    "        CoefsOfInterest[1:] = CoefsOfInterest[1:]/Scaling.loc[(Features, 'sd')]\n",
    "    Selections[d['Family_Name']] = (d['SRModel'], CoefsOfInterest)\n",
    "\n",
    "loc_ContactFreqs = '/Users/slambert/Data/DNAproDB/PFamContactFrequencies/'\n",
    "\n",
    "count = 0\n",
    "for fname, info in Selections.items():\n",
    "    DBDname = tf_families.loc[tf_families['Family_Name']==fname , 'DBDs'][0]\n",
    "    if os.path.isfile(loc_ContactFreqs + DBDname + '.csv'):\n",
    "        print fname, DBDname\n",
    "        coefs = info[1]\n",
    "        cfreqs = pd.read_csv(loc_ContactFreqs + DBDname + '.csv', index_col=0)\n",
    "        cfreqs.index = ['p' + str(x) for x in cfreqs.index]\n",
    "        cfreqs['Coef'] = np.nan\n",
    "        for p, val in coefs.iteritems():\n",
    "            if pd.isnull(val):\n",
    "                cfreqs.loc[p, 'Coef'] = 0\n",
    "            else:\n",
    "                cfreqs.loc[p, 'Coef'] = val\n",
    "        cfreqs['Family_Name'] = fname\n",
    "        cfreqs.reset_index(inplace=True)\n",
    "        if count == 0:\n",
    "            allfreqs = cfreqs.copy()\n",
    "        else:\n",
    "            allfreqs = pd.concat([allfreqs, cfreqs])\n",
    "        count +=1\n",
    "\n",
    "allfreqs.to_csv('JointModelContactFreqs.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
